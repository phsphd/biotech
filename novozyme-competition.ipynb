{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1st Place Public LB Solution - Novozyme Competition\nIn this notebook we present our 1st place public LB solution. Unfortunately it finished in 967th place private LB. Because we selected \"hill climbing\" instead of \"random forest\" and preprocessed hill climbing inappropriately for competition metric. However, if we selected \"random forest\" as our final submission, this solution could have finished 1st place private LB too. Therefore I believe this solution has useful information for research in Enzyme stability prediction, so we present it here.\n\nThe competition task was to predict thermostability of enzyme variants for one specific wildtype protein. The public leaderboard was the first half (i.e. first 107 residuals) and the private leaderboard was the second half (i.e. second 98 residuals). The amino acid sequence is displayed below. My detailed explanation is published [here][1]. Overview is the following 3 bullet points:\n\n* Generate as many strong single model/features as possible\n* Probe LB to acquire the public test labels via mathematical optimization\n* Train one final model using my single models as features and public test labels as targets\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Dec-2022/half2.png)\n\n[1]: https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/376116","metadata":{}},{"cell_type":"markdown","source":"# Features / Single Models\nThe following is a list of my features and single models that I discovered and/or developed during the competition. Most come from public notebooks. The table below lists each feature/model's public and private LB score and a link to the origin of the idea.\n\n## Strong Features/Models with LB 300+\n| Model | Public LB | Private LB | links |\n| --- | --- | --- | --- |\n| Thermonet v2 | 495 | 460 | [link][3] |\n| Rosetta Energy | 471 | 438 | [link][4] |\n| RASP | 454 | 416 | [link][5] [link][6] |\n| RSASA wt | 449 | 484 | [link][2] [link][22] |\n| RSASA mut | 428 | 477 | [link][15] |\n| SASA wt+mut | 420 | 487 | [link][2] |\n| RMSD | 410 | 350 | [link][7] |\n| XGB v2 | 408 | 394 | [link][8] |\n| SASA wt | 407 | 466 | [link][1] [link][2] |\n| XGB v1 | 397 | 400 | [link][8] |\n| SA apolar | 395 | 406 | [link][2] |\n| Thermonet v1 | 386 | 428 | [link][9] |\n| SA sidechain | 377 | 342 | [link][2] |\n| deepDDG | 364 | 429 | [link][10] [link][11] |\n| DeMask score | 363 | 340 | [link][12] |\n| gemme | 352 | 260 | [link][13] |\n| DeMask log2f var | 323 | 259 | [link][30] |\n| residue depth wt | 322 | 326 | [link][14] |\n| residue depth mut | 320 | 338 | [link][14] |\n| continuous Blosum62 | 315 | 207 | [link][15] |\n| ESM finetune | 308 | 352 | [link][16] |\n| SA backbone | 297 | 420 | [link][2] |\n| pLDDT diff | 297 | 287 | [link][17] [link][18] |\n| inps 3d | 293 | 318 | [link][19] |\n| duet | 291 | 406 | [link][20] |\n| pLDDT | 291 | 209 | [link][21] |\n| DeMask entropy | 285 | 340| [link][30] |\n\n## Weak Features/Models with LB 200-300\n| Model | Public LB | Private LB | links |\n| --- | --- | --- | --- |\n| ddgun seq | 281 | 231 | [link][15] |\n| sdm | 277 | 328 | [link][23] |\n| inps seq | 256 | 244 | [link][24] |\n| imut 3d | 246 | 208 | [link][22] |\n| ESM entropy | 238 | 134 | [link][26] |\n| DeMask sub matrix | 236 | 211 | [link][30] |\n| ESM | 234 | 107 | [link][25] |\n| dynamut | 226 | 357 | [link][27] |\n| imut seq | 224 | 191 | [link][22] |\n| mCSM | 214 | 384 | [link][28] |\n| ESM mut prob | 210 | 065 | [link][26] |\n| blosum 95 | 203 | 141 ||\n| ddgun 3d | 199 | 216 | [link][29] |\n| blosum 100 | 197 | 135 ||\n\n[1]: https://www.kaggle.com/code/roberthatch/nesp-alphafold-getarea-exploration\n[2]: https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/357899\n[3]: https://www.kaggle.com/code/vslaykovsky/nesp-thermonet-v2\n[4]: https://www.kaggle.com/code/shlomoron/nesp-relaxed-rosetta-scores\n[5]: https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/368270\n[6]: https://www.kaggle.com/code/sgreiner/novo-esp-rasp-baseline\n[7]: https://www.kaggle.com/code/oxzplvifi/rmsd-from-molecular-dynamics\n[8]: https://www.kaggle.com/code/cdeotte/xgboost-5000-mutations-200-pdb-files-lb-0-410\n[9]: https://www.kaggle.com/code/vslaykovsky/nesp-thermonet\n[10]: https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/354631\n[11]: https://www.kaggle.com/code/hengck23/lb0-335-deepdgg-server-benchmark\n[12]: https://demask.princeton.edu/query/\n[13]: https://www.kaggle.com/code/geraseva/gemme/notebook\n[14]: https://www.kaggle.com/code/gehallak/nesp-3d-geometry-0-32-lb\n[15]: https://www.kaggle.com/code/homofaberus/novozymes-data-enrichement\n[16]: https://www.kaggle.com/code/cdeotte/protein-bert-finetune-lb-0-20\n[17]: https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/361816\n[18]: https://www.kaggle.com/code/cdeotte/difference-features-lb-0-600\n[19]: https://inpsmd.biocomp.unibo.it/inpsSuite/default/index3D\n[20]: https://www.kaggle.com/code/geraseva/duet-tool\n[21]: https://www.kaggle.com/code/chinartist/fork-nesp-an-error-was-corrected\n[22]: https://folding.biofold.org/i-mutant+/index.html\n[23]: https://www.kaggle.com/code/geraseva/sdm-tool\n[24]: https://inpsmd.biocomp.unibo.it/welcome/default/index\n[25]: https://www.kaggle.com/code/kaggleqrdl/esm-quick-start-lb237\n[26]: https://www.kaggle.com/code/kaggleqrdl/esm-quick-start-lb237/comments#1981223\n[27]: https://www.kaggle.com/code/geraseva/dynamut2\n[28]: https://www.kaggle.com/code/geraseva/mcsmtool\n[29]: https://folding.biofold.org/ddgun/index.html\n[30]: https://demask.princeton.edu/about/","metadata":{}},{"cell_type":"code","source":"import os, pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\n\nPATH = '/kaggle/input/novozyme-submission-files/'\nfiles = os.listdir(PATH)\nprint(f'We have {len(files)} features/models.')","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:36.960774Z","iopub.execute_input":"2023-01-08T02:40:36.961197Z","iopub.status.idle":"2023-01-08T02:40:36.996608Z","shell.execute_reply.started":"2023-01-08T02:40:36.961119Z","shell.execute_reply":"2023-01-08T02:40:36.994425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE NUMPY TRAIN ARRAY WITH FEATURES/MODELS\nx_train = []\nfor f in files:\n    df = pd.read_csv(PATH+f)\n    x_train.append(df.tm.values)\nx_train = np.stack(x_train).T\nx_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:36.998564Z","iopub.execute_input":"2023-01-08T02:40:36.998847Z","iopub.status.idle":"2023-01-08T02:40:37.270726Z","shell.execute_reply.started":"2023-01-08T02:40:36.99882Z","shell.execute_reply":"2023-01-08T02:40:37.269839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DISPLAY SOME FEATURES/MODELS\nfor j in range(1):\n    plt.figure(figsize=(20,5))\n    for k in range(3):\n        plt.subplot(1,3,k+1)\n        plt.hist(x_train[:,k+3*j],bins=100)\n        plt.title(files[k+3*j])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:37.272207Z","iopub.execute_input":"2023-01-08T02:40:37.27258Z","iopub.status.idle":"2023-01-08T02:40:38.142349Z","shell.execute_reply.started":"2023-01-08T02:40:37.272544Z","shell.execute_reply":"2023-01-08T02:40:38.140778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Objective Function\nWhen using hill climbing, we need an objective function to optimize. This is our metric computation. In this competition, we fit to the public leaderboard. We accomplish this locally by using our LB 865 submission. We can estimate LB score locally by computing the correlation of an ensemble with our LB 865 submission and then multiplying by 858.3 and subtracting -0.1 (values computed using linear regression from existing submissions)","metadata":{}},{"cell_type":"code","source":"from scipy.stats import spearmanr\n\n# LOAD TARGETS\nPATH2 = '/kaggle/input/novozyme-public-lb-865-submission/'\ny_train = pd.read_csv(PATH2 + 'Novozyme_Public_LB_865.csv').tm.values\nplt.title('LB 865 Submission Preds')\nplt.hist(y_train,bins=100)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:38.144841Z","iopub.execute_input":"2023-01-08T02:40:38.145335Z","iopub.status.idle":"2023-01-08T02:40:39.256567Z","shell.execute_reply.started":"2023-01-08T02:40:38.145303Z","shell.execute_reply":"2023-01-08T02:40:39.255534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOCATE THE PUBLIC LB\nmask = np.array( [False]*len(y_train) )\nmask[:541] = True\nmask[-656:] = True\n\n# REMOVE DELETE MUTATIONS\ntest = pd.read_csv('/kaggle/input/novozymes-enzyme-stability-prediction/test.csv')\ndelete = test.loc[test.protein_sequence.str.len()<221].index.values\nmask[delete] = False\n\n# OBJECTIVE FUCTION - SPEARMAN CORRELATION\nslope,intercept = (858.3108981818078, -0.09320121696697797)\ndef compute_metric(p, apply_mask=True):\n    true = y_train[mask]\n    if apply_mask: pred = p[mask]\n    else: pred = p\n    r = spearmanr( true,pred ).correlation\n    return slope * r + intercept","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:39.257906Z","iopub.execute_input":"2023-01-08T02:40:39.258288Z","iopub.status.idle":"2023-01-08T02:40:39.291189Z","shell.execute_reply.started":"2023-01-08T02:40:39.258249Z","shell.execute_reply":"2023-01-08T02:40:39.289719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hill Climbing\nFor my final solution, I used `NORMALIZE = True` and `RANK = False` as preprocessing for hill climbing. This produced a submission with the same public LB score as `NORMALIZE = False` and `RANK = True` but since the competition metric is Spearman correlation, the former has a much lower private LB score. When using hill climbing make sure to preprocess correctly based on competition metric. See image below:\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Dec-2022/hc_met2.png)\n\n## Step 1: Prepare Preds","metadata":{}},{"cell_type":"code","source":"from scipy.stats import rankdata\n\nNORMALIZE = False\nRANK = True\n\nfor k,name in enumerate(files):\n    if RANK:\n        x_train[:,k] = rankdata( x_train[:,k] )\n    if NORMALIZE:\n        p = x_train[:,k]\n        mn = np.mean(p)\n        sd = np.std(p)\n        x_train[:,k] = (p-mn)/sd","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:39.292502Z","iopub.execute_input":"2023-01-08T02:40:39.292885Z","iopub.status.idle":"2023-01-08T02:40:39.312237Z","shell.execute_reply.started":"2023-01-08T02:40:39.292848Z","shell.execute_reply":"2023-01-08T02:40:39.310635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Find Best Model","metadata":{}},{"cell_type":"code","source":"best_score = 0\nbest_index = -1\n\nfor k,name in enumerate( files ):\n    s = compute_metric(x_train[:,k])\n    if s > best_score:\n        best_score = s\n        best_index = k\n    print(f'Spearman {s:0.1f} {name}') \nprint()\nprint(f'Best single model is {files[best_index]} with Spearman = {best_score:0.1f}')","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:39.31447Z","iopub.execute_input":"2023-01-08T02:40:39.314856Z","iopub.status.idle":"2023-01-08T02:40:39.369202Z","shell.execute_reply.started":"2023-01-08T02:40:39.314823Z","shell.execute_reply":"2023-01-08T02:40:39.367709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Hill Climb Algorithm\nHill climbing begins with the best model. Then it tries adding each model with a variety of weights. It keeps the original model and the one new model which achieves the best two-model ensemble validation score. Then it tries adding each model again with a variety of weights. It then keeps the 3 models which do best. Then it tries add each model again. It repeats this procedure until the ensemble validation score does not improve or we reach our `MAX_MODELS` threshold. Hill climbing works great and has helped win many competitions. \n\nNote in Novozyme comp, hill climbing would not have been the best. Hill climbing is just linear regression. As such, it can not learn interactions and/or non-linearity like random forest. In Novozyme competition, random forest was a better choice to use.","metadata":{}},{"cell_type":"code","source":"USE_NEGATIVE_WGT = True\nMAX_MODELS = 1000\nTOL = 1e-5","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:39.370926Z","iopub.execute_input":"2023-01-08T02:40:39.371343Z","iopub.status.idle":"2023-01-08T02:40:39.378034Z","shell.execute_reply.started":"2023-01-08T02:40:39.371297Z","shell.execute_reply":"2023-01-08T02:40:39.376116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = [best_index]\nold_best_score = best_score\nbest_ensemble = x_train[:,best_index]\n\nmodels = [best_index]\nweights = []\nmetrics = [best_score]\n\nfor kk in range(1_000_000):\n\n    best_score = 0\n    best_index = -1\n    best_weight = 0\n\n    for k,ff in enumerate(files):\n        \n        new_model = x_train[:,k]\n        start = -0.50\n        if not USE_NEGATIVE_WGT: start = 0.01\n            \n        for wgt in list(np.arange(start,0.51,0.01)): \n            tmp = (1-wgt) * best_ensemble + wgt * new_model\n            s = compute_metric(tmp)\n            if s > best_score:\n                best_score = s\n                best_index = k\n                best_weight = wgt\n                potential_ensemble = tmp\n    \n    indices.append(best_index)\n    indices = list(np.unique(indices))\n\n    if len(indices)>MAX_MODELS:\n        print(f'=> We reached {MAX_MODELS} models')\n        indices = indices[:-1]\n        break\n    \n    if best_score - old_best_score < TOL: \n        print(f'=> We reached tolerance {TOL}')\n        break\n        \n    print(kk,'New best spearman',best_score,'adding',files[best_index],'with weight',f'{best_weight:0.3f}')\n        \n    models.append(best_index)\n    weights.append(best_weight)\n    metrics.append(best_score)\n    best_ensemble = potential_ensemble\n    old_best_score = best_score","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:40:39.379885Z","iopub.execute_input":"2023-01-08T02:40:39.380239Z","iopub.status.idle":"2023-01-08T02:42:24.835197Z","shell.execute_reply.started":"2023-01-08T02:40:39.380213Z","shell.execute_reply":"2023-01-08T02:42:24.833749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Convert Incremental Weights to Absolute Weights\nIn the hill climbing algorithm, the same model may be blended in at different iterations. Therefore we must aggregate all weights to create a list of ensemble weights where each model has one weight and all weights add up to 1.","metadata":{}},{"cell_type":"code","source":"wgt = np.array([1])\nfor w in weights:\n    wgt = wgt*(1-w)\n    wgt = np.concatenate([wgt,np.array([w])])\n    \nrows = []\nt = 0\nfor m,w,s in zip(models,wgt,metrics):\n    name = files[m]\n    dd = {}\n    dd['weight'] = w\n    dd['model'] = name\n    rows.append(dd)\n    t += float( f'{w:.3f}' )\n    \ndf = pd.DataFrame(rows)\ndf = df.groupby('model').agg('sum').reset_index().sort_values('weight',ascending=False)\ndf = df.reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:24.838203Z","iopub.execute_input":"2023-01-08T02:42:24.838495Z","iopub.status.idle":"2023-01-08T02:42:24.870669Z","shell.execute_reply.started":"2023-01-08T02:42:24.83847Z","shell.execute_reply":"2023-01-08T02:42:24.869436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Ensemble weights sum to',df.weight.sum())\ndf.to_csv('ensemble_weights.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:24.872311Z","iopub.execute_input":"2023-01-08T02:42:24.873098Z","iopub.status.idle":"2023-01-08T02:42:24.883226Z","shell.execute_reply.started":"2023-01-08T02:42:24.873056Z","shell.execute_reply":"2023-01-08T02:42:24.88179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Make Submission","metadata":{}},{"cell_type":"code","source":"x_map = {x:y for x,y in zip(files,np.arange(len(files)))}\npred = x_train[:, x_map[df.model.iloc[0]] ] * df.weight.iloc[0]\nfor k in range(1,len(df)):\n    pred += x_train[:, x_map[df.model.iloc[k]] ] * df.weight.iloc[k]\nm = compute_metric(pred)\nprint(f'Ensemble has spearman {m:0.1f}')","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:24.884826Z","iopub.execute_input":"2023-01-08T02:42:24.88523Z","iopub.status.idle":"2023-01-08T02:42:24.898768Z","shell.execute_reply.started":"2023-01-08T02:42:24.885198Z","shell.execute_reply":"2023-01-08T02:42:24.897943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(PATH+files[1])\nsub.tm = pred\nsub.to_csv('submission_hill_climb.csv',index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:24.900054Z","iopub.execute_input":"2023-01-08T02:42:24.900657Z","iopub.status.idle":"2023-01-08T02:42:24.92177Z","shell.execute_reply.started":"2023-01-08T02:42:24.900623Z","shell.execute_reply":"2023-01-08T02:42:24.920222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Dec-2022/hc1.png)","metadata":{}},{"cell_type":"markdown","source":"# Random Forest\nDuring the competition, we submitted `RandomForestRegressor()` with default hyperparameters and achieved public LB 0.829 and private LB 0.546 **1st Place private LB** with screen shot posted [here][1]. But we **did not select it** for fear of overfitting public LB and doing poorly on private LB. \n\nAfter the competition ended, we submitted random forest with different `max_depth` equal `3, 4, 5, 6` and discovered they all achieve private LB 0.545 or better. The parameter `max_depth = 4` works best and is displayed below.\n\n[1]: https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/375911#2085245","metadata":{}},{"cell_type":"code","source":"public_test = x_train[mask,]\npublic_target = y_train[mask]\npublic_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:24.923426Z","iopub.execute_input":"2023-01-08T02:42:24.923786Z","iopub.status.idle":"2023-01-08T02:42:24.930811Z","shell.execute_reply.started":"2023-01-08T02:42:24.923757Z","shell.execute_reply":"2023-01-08T02:42:24.929815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(max_depth=4,n_estimators=100,random_state=77)\nmodel.fit( public_test, public_target )\np = model.predict( public_test )\nm = compute_metric(p, apply_mask=False)\nprint('Our train metric is spearman',m)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:24.933464Z","iopub.execute_input":"2023-01-08T02:42:24.934351Z","iopub.status.idle":"2023-01-08T02:42:26.149414Z","shell.execute_reply.started":"2023-01-08T02:42:24.934298Z","shell.execute_reply":"2023-01-08T02:42:26.14801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(x_train)\npred.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:26.150587Z","iopub.execute_input":"2023-01-08T02:42:26.150912Z","iopub.status.idle":"2023-01-08T02:42:26.17393Z","shell.execute_reply.started":"2023-01-08T02:42:26.150878Z","shell.execute_reply":"2023-01-08T02:42:26.172899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(PATH+files[1])\nsub.tm = pred\nsub.to_csv('submission_random_forest.csv',index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:26.175638Z","iopub.execute_input":"2023-01-08T02:42:26.176041Z","iopub.status.idle":"2023-01-08T02:42:26.19619Z","shell.execute_reply.started":"2023-01-08T02:42:26.176004Z","shell.execute_reply":"2023-01-08T02:42:26.195026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Dec-2022/rf1.png)","metadata":{}},{"cell_type":"markdown","source":"# Random Forest EDA\nBelow we display random forest feature importance and the first 4 trees of the 100 forest trees.","metadata":{}},{"cell_type":"code","source":"importances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nforest_importances = pd.Series(importances, index=files)\nplt.figure(figsize=(20,5))\nforest_importances.plot.bar(yerr=std)\nplt.title(\"Random Forest Feature importances\",size=18)\nplt.ylabel(\"Mean decrease in impurity\",size=16)\nplt.xticks(fontsize=14, rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:26.197567Z","iopub.execute_input":"2023-01-08T02:42:26.198021Z","iopub.status.idle":"2023-01-08T02:42:26.673778Z","shell.execute_reply.started":"2023-01-08T02:42:26.197992Z","shell.execute_reply":"2023-01-08T02:42:26.672996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\nimport graphviz\n\nfor k in range(4):\n    print('#'*25)\n    print('### Random Forest Tree',k+1)\n    print('#'*25)\n    \n    clf = model.estimators_[k]\n    dot_data = tree.export_graphviz(clf, out_file=None, \n                                    feature_names=files,  \n                                    filled=True)\n\n    # Draw graph\n    graph = graphviz.Source(dot_data, format=\"png\")  \n    display( graph )\n    print('\\n\\n\\n\\n\\n\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2023-01-08T02:42:26.675032Z","iopub.execute_input":"2023-01-08T02:42:26.676202Z","iopub.status.idle":"2023-01-08T02:42:27.788405Z","shell.execute_reply.started":"2023-01-08T02:42:26.676165Z","shell.execute_reply":"2023-01-08T02:42:27.786924Z"},"trusted":true},"execution_count":null,"outputs":[]}]}